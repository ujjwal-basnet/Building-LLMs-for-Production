{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "809c81e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6b98eecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    }
   ],
   "source": [
    "### load faceboo/opt-1.3b model in 8 bit format to save memory \n",
    "OPT = AutoModelForCausalLM.from_pretrained(\"facebook/opt-1.3b\", load_in_8bit=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-1.3b\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ec9ec3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp= \" The quick brown fox jumps over the lazy dog. The dog is not happy about it.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "995ec5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_tokenized= tokenizer(inp , return_tensors=\"pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "758e1ff5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    2,    20,  2119,  6219, 23602, 13855,    81,     5, 22414,  2335,\n",
       "             4,    20,  2335,    16,    45,  1372,    59,    24,     4]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "34babf2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens= inp.split()\n",
    "len(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f93fb2f",
   "metadata": {},
   "source": [
    "each word is not always one token!\n",
    "\n",
    "Some words get split into multiple subword tokens.\n",
    "\n",
    "E.g., \"dog.\" might split into \"dog\" + \".\".\n",
    "\n",
    "Or \"it.\" → \"it\" + \".\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9b2a27",
   "metadata": {},
   "source": [
    "***********************************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d445f5f",
   "metadata": {},
   "source": [
    "## model archetecture "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a0bfff2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPTModel(\n",
      "  (decoder): OPTDecoder(\n",
      "    (embed_tokens): Embedding(50272, 2048, padding_idx=1)\n",
      "    (embed_positions): OPTLearnedPositionalEmbedding(2050, 2048)\n",
      "    (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "    (layers): ModuleList(\n",
      "      (0-23): 24 x OPTDecoderLayer(\n",
      "        (self_attn): OPTAttention(\n",
      "          (k_proj): Linear8bitLt(in_features=2048, out_features=2048, bias=True)\n",
      "          (v_proj): Linear8bitLt(in_features=2048, out_features=2048, bias=True)\n",
      "          (q_proj): Linear8bitLt(in_features=2048, out_features=2048, bias=True)\n",
      "          (out_proj): Linear8bitLt(in_features=2048, out_features=2048, bias=True)\n",
      "        )\n",
      "        (activation_fn): ReLU()\n",
      "        (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear8bitLt(in_features=2048, out_features=8192, bias=True)\n",
      "        (fc2): Linear8bitLt(in_features=8192, out_features=2048, bias=True)\n",
      "        (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(OPT.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "19af499e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 19])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp_tokenized['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6566405c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(next(OPT.model.parameters()).device)  # Model device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3561ce43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "38a14e8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device= 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e1cbc2eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0407,  0.0519,  0.0574,  ..., -0.0263, -0.0355, -0.0260],\n",
       "         [-0.0464,  0.0228,  0.0339,  ...,  0.0076, -0.0065, -0.0167],\n",
       "         [-0.0455, -0.0236, -0.0121,  ...,  0.0043, -0.0166,  0.0193],\n",
       "         ...,\n",
       "         [ 0.0408,  0.0363,  0.0021,  ..., -0.0348,  0.0170, -0.0540],\n",
       "         [ 0.0046,  0.0081,  0.0311,  ...,  0.0173,  0.0141, -0.0444],\n",
       "         [ 0.0281,  0.0338,  0.0049,  ...,  0.0564,  0.0444, -0.0569]]],\n",
       "       device='cuda:0', dtype=torch.float16, grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_input = OPT.model.decoder.embed_tokens(inp_tokenized['input_ids'].to(device))\n",
    "embedding_input # output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9e18b912",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 19, 2048])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#size \n",
    "embedding_input.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8badd97a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(50272, 2048, padding_idx=1)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## layer  \n",
    "OPT.model.decoder.embed_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a001582e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OPTLearnedPositionalEmbedding(2050, 2048)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#embed positions \n",
    "OPT.model.decoder.embed_positions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fec891e",
   "metadata": {},
   "source": [
    "inp_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2beb6d10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    2,    20,  2119,  6219, 23602, 13855,    81,     5, 22414,  2335,\n",
       "             4,    20,  2335,    16,    45,  1372,    59,    24,     4]],\n",
       "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
       "       device='cuda:0')}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ffb9ade9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-8.1406e-03, -2.6221e-01,  6.0768e-03,  ...,  1.7273e-02,\n",
       "          -5.0621e-03, -1.6220e-02],\n",
       "         [-8.0585e-05,  2.5000e-01, -1.6632e-02,  ..., -1.5419e-02,\n",
       "          -1.7838e-02,  2.4948e-02],\n",
       "         [-9.9411e-03, -1.4978e-01,  1.7557e-03,  ...,  3.7117e-03,\n",
       "          -1.6434e-02, -9.9087e-04],\n",
       "         ...,\n",
       "         [ 6.8130e-03, -4.7180e-02, -6.3515e-03,  ..., -1.2741e-02,\n",
       "           2.6345e-04,  1.1848e-02],\n",
       "         [ 7.9651e-03, -1.4923e-02, -2.2873e-02,  ...,  7.7009e-04,\n",
       "          -3.7445e-02,  1.2596e-02],\n",
       "         [-1.1832e-04, -2.1637e-02, -8.1110e-04,  ...,  5.6190e-03,\n",
       "          -4.0016e-03, -1.0094e-02]]], device='cuda:0', dtype=torch.float16,\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#embed positions \n",
    "embed_pos_input = OPT.model.decoder.embed_positions(inp_tokenized['attention_mask'])\n",
    "embed_pos_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "33d43ed8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OPTLearnedPositionalEmbedding(2050, 2048)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##size\n",
    "OPT.model.decoder.embed_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d80b376a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OPTModel(\n",
       "  (decoder): OPTDecoder(\n",
       "    (embed_tokens): Embedding(50272, 2048, padding_idx=1)\n",
       "    (embed_positions): OPTLearnedPositionalEmbedding(2050, 2048)\n",
       "    (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "    (layers): ModuleList(\n",
       "      (0-23): 24 x OPTDecoderLayer(\n",
       "        (self_attn): OPTAttention(\n",
       "          (k_proj): Linear8bitLt(in_features=2048, out_features=2048, bias=True)\n",
       "          (v_proj): Linear8bitLt(in_features=2048, out_features=2048, bias=True)\n",
       "          (q_proj): Linear8bitLt(in_features=2048, out_features=2048, bias=True)\n",
       "          (out_proj): Linear8bitLt(in_features=2048, out_features=2048, bias=True)\n",
       "        )\n",
       "        (activation_fn): ReLU()\n",
       "        (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear8bitLt(in_features=2048, out_features=8192, bias=True)\n",
       "        (fc2): Linear8bitLt(in_features=8192, out_features=2048, bias=True)\n",
       "        (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OPT.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c5e28311",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0407,  0.0519,  0.0574,  ..., -0.0263, -0.0355, -0.0260],\n",
       "         [-0.0464,  0.0228,  0.0339,  ...,  0.0076, -0.0065, -0.0167],\n",
       "         [-0.0455, -0.0236, -0.0121,  ...,  0.0043, -0.0166,  0.0193],\n",
       "         ...,\n",
       "         [ 0.0408,  0.0363,  0.0021,  ..., -0.0348,  0.0170, -0.0540],\n",
       "         [ 0.0046,  0.0081,  0.0311,  ...,  0.0173,  0.0141, -0.0444],\n",
       "         [ 0.0281,  0.0338,  0.0049,  ...,  0.0564,  0.0444, -0.0569]]],\n",
       "       device='cuda:0', dtype=torch.float16, grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "629cff7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## token embedding + position embedding  =  real embedding\n",
    "embed_position_embeddding= embedding_input + embed_pos_input "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "57135ac9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OPTAttention(\n",
       "  (k_proj): Linear8bitLt(in_features=2048, out_features=2048, bias=True)\n",
       "  (v_proj): Linear8bitLt(in_features=2048, out_features=2048, bias=True)\n",
       "  (q_proj): Linear8bitLt(in_features=2048, out_features=2048, bias=True)\n",
       "  (out_proj): Linear8bitLt(in_features=2048, out_features=2048, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OPT.model.decoder.layers[0].self_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "576fb9b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.0136, -0.0095,  0.0012,  ...,  0.0067, -0.0018,  0.0131],\n",
       "          [-0.0134, -0.0098,  0.0028,  ...,  0.0085,  0.0001,  0.0122],\n",
       "          [-0.0133, -0.0055,  0.0040,  ...,  0.0097,  0.0019,  0.0138],\n",
       "          ...,\n",
       "          [-0.0122, -0.0092,  0.0058,  ...,  0.0094,  0.0015,  0.0094],\n",
       "          [-0.0123, -0.0095,  0.0059,  ...,  0.0095,  0.0016,  0.0092],\n",
       "          [-0.0122, -0.0098,  0.0059,  ...,  0.0093,  0.0016,  0.0093]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<MatMul8bitLtBackward>),\n",
       " None,\n",
       " None)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OPT.model.decoder.layers[0].self_attn(embed_position_embeddding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "65f7f7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_states, _ , _ = OPT.model.decoder.layers[0].self_attn(embed_position_embeddding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "58916d86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0136, -0.0095,  0.0012,  ...,  0.0067, -0.0018,  0.0131],\n",
       "         [-0.0134, -0.0098,  0.0028,  ...,  0.0085,  0.0001,  0.0122],\n",
       "         [-0.0133, -0.0055,  0.0040,  ...,  0.0097,  0.0019,  0.0138],\n",
       "         ...,\n",
       "         [-0.0122, -0.0092,  0.0058,  ...,  0.0094,  0.0015,  0.0094],\n",
       "         [-0.0123, -0.0095,  0.0059,  ...,  0.0095,  0.0016,  0.0092],\n",
       "         [-0.0122, -0.0098,  0.0059,  ...,  0.0093,  0.0016,  0.0093]]],\n",
       "       device='cuda:0', dtype=torch.float16, grad_fn=<MatMul8bitLtBackward>)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "881f7837",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 19, 2048])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_states.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a3cb3599",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OPTAttention(\n",
       "  (k_proj): Linear8bitLt(in_features=2048, out_features=2048, bias=True)\n",
       "  (v_proj): Linear8bitLt(in_features=2048, out_features=2048, bias=True)\n",
       "  (q_proj): Linear8bitLt(in_features=2048, out_features=2048, bias=True)\n",
       "  (out_proj): Linear8bitLt(in_features=2048, out_features=2048, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#LAYERS\n",
    "OPT.model.decoder.layers[0].self_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "3658621b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## bart "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "6893b65b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BartModel(\n",
       "  (shared): BartScaledWordEmbedding(50265, 1024, padding_idx=1)\n",
       "  (encoder): BartEncoder(\n",
       "    (embed_tokens): BartScaledWordEmbedding(50265, 1024, padding_idx=1)\n",
       "    (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
       "    (layers): ModuleList(\n",
       "      (0-11): 12 x BartEncoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (activation_fn): GELUActivation()\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (decoder): BartDecoder(\n",
       "    (embed_tokens): BartScaledWordEmbedding(50265, 1024, padding_idx=1)\n",
       "    (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
       "    (layers): ModuleList(\n",
       "      (0-11): 12 x BartDecoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (activation_fn): GELUActivation()\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer \n",
    "BART = AutoModel.from_pretrained('facebook/bart-large')\n",
    "BART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "55e10a34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ddde451dde249debb78e8cc110faf20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4c8d9c57b164cf999f4dd13a581fc7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe498e0deeab4bd78d49348476df66d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e92ee5fa35c4d5a941fd2315a4930a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "406017b1cc9e405e812f734841c658e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94520ddb84874980aea82ef0b73870d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'summary_text': 'Bennett and Gaga became fast friends and close collaborators. They recorded two albums together, 2014\\'s \"Cheek to Cheek\" and 2021\\'s \"Love for Sale\"'}]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "sum = summarizer(\"\"\"Gaga was best known in the 2010s for pop hits like “Poker Face” and avant-garde experimentation on albums like “Artpop,” and Bennett, a singer who mostly stuck to standards, was in his 80s when the pair met. And yet Bennett and Gaga became fast friends and close collaborators, which they remained until Bennett’s death at 96 on Friday. They recorded two albums together, 2014’s “Cheek to Cheek” and 2021’s “Love for Sale,” which both won Grammys for best traditional pop vocal album.\"\"\", min_length=20, max_length=50)\n",
    "sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f24f4aca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Bennett and Gaga became fast friends and close collaborators. They recorded two albums together, 2014\\'s \"Cheek to Cheek\" and 2021\\'s \"Love for Sale\"'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum[0]['summary_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f07a25c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<transformers.pipelines.text_classification.TextClassificationPipeline at 0x20dfeb917d0>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier= pipeline('text-classification')\n",
    "classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "be76fe70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9998695850372314}]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier(\"This resturent is awesome\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "8bfa2f84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b5c3cf5b6dc433994367a6067736b58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2628fe79c8914943bc905c223f53e1a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gpt= AutoModel.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "9738f90f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bd8bee1a41c4eec80238120c946984b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a8f51f473cd49f39234a78073833160",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertModel(\n",
      "  (embeddings): BertEmbeddings(\n",
      "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "    (position_embeddings): Embedding(512, 768)\n",
      "    (token_type_embeddings): Embedding(2, 768)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): BertEncoder(\n",
      "    (layer): ModuleList(\n",
      "      (0-11): 12 x BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSdpaSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pooler): BertPooler(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (activation): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "BERT = AutoModel.from_pretrained('bert-base-uncased') \n",
    "print(BERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "89f3a4c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2Model(\n",
       "  (wte): Embedding(50257, 768)\n",
       "  (wpe): Embedding(1024, 768)\n",
       "  (drop): Dropout(p=0.1, inplace=False)\n",
       "  (h): ModuleList(\n",
       "    (0-11): 12 x GPT2Block(\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPT2Attention(\n",
       "        (c_attn): Conv1D(nf=2304, nx=768)\n",
       "        (c_proj): Conv1D(nf=768, nx=768)\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): GPT2MLP(\n",
       "        (c_fc): Conv1D(nf=3072, nx=768)\n",
       "        (c_proj): Conv1D(nf=768, nx=3072)\n",
       "        (act): NewGELUActivation()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AutoModel.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b12d7e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e4b97deb",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a623809a",
   "metadata": {},
   "source": [
    "import numpy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "131a3064",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "def self_attention(query, key, value, mask=None):\n",
    "    ## compute attention score \n",
    "    np.dot(query, key.T)\n",
    "    \n",
    "    if mask is not None : \n",
    "        ## apply mask by setting masked position to large negative value \n",
    "        scores = scores  + -1e9\n",
    "        \n",
    "    ##apply softmax \n",
    "    attention_weights= np.exp(scores) / np.sum(np.exp(scores) , axis= -1 ,keepdims= True )\n",
    "    \n",
    "    # compute weighted sum of value vectors \n",
    "    output= np.dot(attention_weights, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cece91fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "44899230",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from dotenv import load_dotenv \n",
    "\n",
    "load_dotenv() #load variables form env \n",
    "api_key= os.getenv(\"COHERE_key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "fe5d86c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cohere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "b14b66e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'aqhwRQAfBFsQb2pvIdXsrOgbTgscshNDqRQ48Hcz'"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "0d4c5483",
   "metadata": {},
   "outputs": [],
   "source": [
    "co = cohere.Client(api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "63b17b96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'USER', 'message': 'who discoverd gravity'},\n",
       " {'role': 'CHATBOT',\n",
       "  'message': 'The man who widely credited with discovering gravity is Sir Isac Newton'}]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_history=[\n",
    "{\n",
    "    \"role\" : \"USER\" , \n",
    "    \"message\" : \"who discoverd gravity\" \n",
    "},\n",
    "\n",
    " {\n",
    "     \"role\" : \"CHATBOT\", \n",
    "     \"message\": \"The man who widely credited with discovering gravity is Sir Isac Newton\"\n",
    "     \n",
    " }\n",
    "] \n",
    "\n",
    "chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "b2c2046c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'what year was he bron ?'"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message = \"what year was he bron ?\" \n",
    "message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "d29bf31b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NonStreamedChatResponse(text=\"Sir Isaac Newton was born on December 25, 1642. At the time of Newton's birth, England used the Julian calendar. However, when England adopted the Gregorian calendar in 1752, his birthday became January 4, 1643.\", generation_id='14f5f38a-ed4f-4635-8af9-2882ed361510', response_id='a22faa0e-a3a9-419b-b029-359025ae7c0b', citations=[ChatCitation(start=29, end=40, text='December 25', document_ids=['internet_search_9_1_0:0', 'internet_search_11_1_0:0', 'internet_search_4_1_0:0'], type='TEXT_CONTENT'), ChatCitation(start=42, end=47, text='1642.', document_ids=['internet_search_9_1_0:0', 'internet_search_11_1_0:0', 'internet_search_15_1_0:0', 'internet_search_4_1_0:0'], type='TEXT_CONTENT'), ChatCitation(start=79, end=112, text='England used the Julian calendar.', document_ids=['internet_search_11_1_0:0'], type='TEXT_CONTENT'), ChatCitation(start=127, end=173, text='England adopted the Gregorian calendar in 1752', document_ids=['internet_search_11_1_0:0'], type='TEXT_CONTENT'), ChatCitation(start=195, end=211, text='January 4, 1643.', document_ids=['internet_search_9_1_0:0', 'internet_search_11_1_0:0'], type='TEXT_CONTENT')], documents=[{'id': 'internet_search_9_1_0:0', 'snippet': \"Isaac NewtonIsaac Newton, oil on canvas by Sir Godfrey Kneller, 1702; in the National Portrait Gallery, London. Newton's discoveries in physics and mathematics revolutionized science.(more) Isaac Newton (born December 25, 1642 [January 4, 1643, New Style], Woolsthorpe, Lincolnshire, England—died March 20 [March 31], 1727, London) was an English physicist and mathematician who was the culminating figure of the Scientific Revolution of the 17th century. In optics, his discovery of the composition of white light integrated the phenomena of colours into the science of light and laid the foundation for modern physical optics. In mechanics, his three laws of motion, the basic principles of modern physics, resulted in the formulation of the law of universal gravitation.\", 'timestamp': '1999-07-26T00:00:00', 'title': 'Isaac Newton | Biography, Facts, Discoveries, Laws, & Inventions | Britannica', 'url': 'https://www.britannica.com/biography/Isaac-Newton'}, {'id': 'internet_search_11_1_0:0', 'snippet': \"Sir Isaac Newton contributed significantly to the field of science over his lifetime. He invented · calculus and provided a clear understanding of optics. But his most significant work had to do with forces, and specifically with the development of a universal law of gravitation and his ... Isaac Newton was born on Christmas Day to a poor farming family in Woolsthorpe, England, in 1642. At the time of Newton's birth England used the Julian calendar, however, when England adopted the Gregorian calendar in 1752, his birthday became 4th January 1643.  · Isaac Newton arrived in the world only a few months after his father, Isaac Newton Sr, had died.\", 'timestamp': '2023-06-06T15:20:35', 'title': 'Sir Isaac Newton biography — Inventions, laws and quotes | Space', 'url': 'https://www.space.com/15898-isaac-newton.html'}, {'id': 'internet_search_4_1_0:0', 'snippet': \"Newton was not the first of the age of reason. He was the last of the magicians, the last of the Babylonians and Sumerians, the last great mind which looked out on the visible and intellectual world with the same eyes as those who began to build our intellectual inheritance rather less than 10,000 years ago. Isaac Newton, a posthumous child born with no father on Christmas Day, 1642, was the last wonderchild to whom the Magi could do sincere and appropriate homage. ... Of an estimated ten million words of writing in Newton's papers, about one million deal with alchemy.\", 'timestamp': '2025-07-09T12:28:19', 'title': 'Isaac Newton - Wikipedia', 'url': 'https://en.wikipedia.org/wiki/Isaac_Newton'}, {'id': 'internet_search_15_1_0:0', 'snippet': 'Isaac Newton was born in 1642 in England. His father had died two months before his birth. When Isaac was three his mother remarried, and Isaac remained with his grandmother. He was not interested in the family farm, so he was sent to Cambridge University to study. Isaac was born just a short time after the death of Galileo, one of the greatest scientists of all time. Galileo had proved that the planets revolve around the sun, not the earth as people thought at the time. Isaac Newton was very interested in the discoveries of Galileo and others. Isaac thought the universe worked like a machine and that a few simple laws governed it.', 'timestamp': '', 'title': 'Newton (1642) - Energy Kids: U.S. Energy Information Administration (EIA)', 'url': 'https://www.eia.gov/kids/history-of-energy/famous-people/newton.php'}], is_search_required=None, search_queries=None, search_results=[ChatSearchResult(search_query=ChatSearchQuery(text='{\"tool_name\":\"internet_search\",\"parameters\":{\"query\":\"sir isaac newton year of birth\"}}', generation_id=None), connector=ChatSearchResultConnector(id='internet_search'), document_ids=['internet_search_0_1_0:1', 'internet_search_4_1_0:0', 'internet_search_9_1_0:0', 'internet_search_11_1_0:0', 'internet_search_15_1_0:0'], error_message=None, continue_on_failure=None)], finish_reason='COMPLETE', tool_calls=[ToolCall(name='internet_search', parameters={'query': 'sir isaac newton year of birth'})], chat_history=[UserMessage(role='USER', message='who discoverd gravity', tool_calls=None), ChatbotMessage(role='CHATBOT', message='The man who widely credited with discovering gravity is Sir Isac Newton', tool_calls=None), UserMessage(role='USER', message='what year was he bron ?', tool_calls=None), ChatbotMessage(role='CHATBOT', message='I will search for the year Sir Isaac Newton was born.', tool_calls=[ToolCall(name='internet_search', parameters={'query': 'sir isaac newton year of birth'})]), ToolMessage(role='TOOL', tool_results=[ToolResult(call=ToolCall(name='internet_search', parameters={'query': 'sir isaac newton year of birth'}), outputs=[{'snippet': 'New Scientist called Newton \"the supreme genius and most enigmatic character in the history of science\". The philosopher and historian David Hume also declared that Newton was \"the greatest and rarest genius that ever arose for the ornament and instruction of the species\". In his home of Monticello, Thomas Jefferson, a Founding Father and President of the United States, kept portraits of John Locke, Sir Francis Bacon, and Newton, whom he described as \"the three greatest men that have ever lived, without any exception\", and who he credited with laying \"the foundation of those superstructures which have been raised in the Physical and Moral sciences\". Newton has further been called \"the towering figure of the Scientific Revolution\" and that \"In a period rich with outstanding thinkers, Newton was simply the most outstanding.\" The polymath Johann Wolfgang von Goethe labeled Newton\\'s birth as the \"Christmas of the modern age\".', 'timestamp': '2025-07-09T12:28:19', 'title': 'Isaac Newton - Wikipedia', 'url': 'https://en.wikipedia.org/wiki/Isaac_Newton'}, {'snippet': 'Newton himself often told the story that he was inspired to formulate his theory of gravitation by watching the fall of an apple from a tree. The story is believed to have passed into popular knowledge after being related by Catherine Barton, Newton\\'s niece, to Voltaire. Voltaire then wrote in his Essay on Epic Poetry (1727), \"Sir Isaac Newton walking in his gardens, had the first thought of his system of gravitation, upon seeing an apple falling from a tree.\" Although it has been said that the apple story is a myth and that he did not arrive at his theory of gravity at any single moment, acquaintances of Newton (such as William Stukeley, whose manuscript account of 1752 has been made available by the Royal Society) do in fact confirm the incident, though not the apocryphal version that the apple actually hit Newton\\'s head. Stukeley recorded in his Memoirs of Sir Isaac Newton\\'s Life a conversation with Newton in Kensington on 15 April 1726: we went into the garden, & drank thea under t', 'timestamp': '2025-07-09T12:28:19', 'title': 'Isaac Newton - Wikipedia', 'url': 'https://en.wikipedia.org/wiki/Isaac_Newton'}, {'snippet': 'A statue of Isaac Newton, looking at an apple at his feet, can be seen at the Oxford University Museum of Natural History. A large bronze statue, Newton, after William Blake, by Eduardo Paolozzi, dated 1995 and inspired by Blake\\'s etching, dominates the piazza of the British Library in London. A bronze statue of Newton was erected in 1858 in the centre of Grantham where he went to school, prominently standing in front of Grantham Guildhall. The still-surviving farmhouse at Woolsthorpe By Colsterworth is a Grade I listed building by Historic England through being his birthplace and \"where he discovered gravity and developed his theories regarding the refraction of light\". It is held by European philosophers of the Enlightenment and by historians of the Enlightenment that Newton\\'s publication of the Principia was a turning point in the Scientific Revolution and started the Enlightenment.', 'timestamp': '2025-07-09T12:28:19', 'title': 'Isaac Newton - Wikipedia', 'url': 'https://en.wikipedia.org/wiki/Isaac_Newton'}, {'snippet': 'In April 1667, Newton returned to the University of Cambridge, and in October he was elected as a fellow of Trinity. Fellows were required to take holy orders and be ordained as Anglican priests, although this was not enforced in the Restoration years, and an assertion of conformity to the Church of England was sufficient. He made the commitment that \"I will either set Theology as the object of my studies and will take holy orders when the time prescribed by these statutes [7 years] arrives, or I will resign from the college.\" Up until this point he had not thought much about religion and had twice signed his agreement to the Thirty-nine Articles, the basis of Church of England doctrine. By 1675 the issue could not be avoided, and his unconventional views stood in the way. His academic work impressed the Lucasian Professor Isaac Barrow, who was anxious to develop his own religious and administrative potential (he became master of Trinity College two years later); in 1669, Newton succee', 'timestamp': '2025-07-09T12:28:19', 'title': 'Isaac Newton - Wikipedia', 'url': 'https://en.wikipedia.org/wiki/Isaac_Newton'}, {'snippet': \"Newton was not the first of the age of reason. He was the last of the magicians, the last of the Babylonians and Sumerians, the last great mind which looked out on the visible and intellectual world with the same eyes as those who began to build our intellectual inheritance rather less than 10,000 years ago. Isaac Newton, a posthumous child born with no father on Christmas Day, 1642, was the last wonderchild to whom the Magi could do sincere and appropriate homage. ... Of an estimated ten million words of writing in Newton's papers, about one million deal with alchemy. Many of Newton's writings on alchemy are copies of other manuscripts, with his own annotations. Alchemical texts mix artisanal knowledge with philosophical speculation, often hidden behind layers of wordplay, allegory, and imagery to protect craft secrets. Some of the content contained in Newton's papers could have been considered heretical by the church.\", 'timestamp': '2025-07-09T12:28:19', 'title': 'Isaac Newton - Wikipedia', 'url': 'https://en.wikipedia.org/wiki/Isaac_Newton'}, {'snippet': 'Newton went home. It was during this time that Newton started to pursue his own ideas on math, physics, optics and astronomy. By 1666 he had completed his early work on his three laws of motion. The university reopened and Newton took a fellowship in order to obtain his masters degree. As the years progressed, Newton completed his work on universal gravitation, diffraction of light, centrifugal force, centripetal force, inverse-square law, bodies in motion and the variations in tides due to gravity. His impressive body of work made him a leader in scientific research. However, in 1679 his work came to standstill after he suffered a nervous breakdown. Upon regaining his health Newton returned to the university. He became a leader against what he saw as an attack on the university by King James II. The king wanted only Roman Catholics to be in positions of power in government and academia.', 'timestamp': '', 'title': 'Sir Isaac Newton', 'url': 'https://starchild.gsfc.nasa.gov/docs/StarChild/whos_who_level2/newton.html'}, {'snippet': \"Throughout Newton's career he was torn between his desire for fame and his fear of criticism. His overwhelming fear of criticism caused him to resist immediate publication of his work. As a consequence Newton often felt compelled to defend his work against plagiarism. One such dispute arose over calculus. Though Newton had been the first to derive calculus as a mathematical approach, Gottfried Leibniz was the first one to widely disseminate the concept throughout Europe. The dispute with Leibniz dominated the last years of his life. Newton died in 1727. The StarChild site is a service of the High Energy Astrophysics Science Archive Research Center (HEASARC), within the Astrophysics Science Division (ASD) at NASA/ GSFC. StarChild Authors: The StarChild Team StarChild Graphics & Music: Acknowledgments StarChild Project Leader: Dr. Laura A. Whitlock Curator: J.D.\", 'timestamp': '', 'title': 'Sir Isaac Newton', 'url': 'https://starchild.gsfc.nasa.gov/docs/StarChild/whos_who_level2/newton.html'}, {'snippet': \"Trinity College Dublin - School of mathematics - Biography of Sir Isaac Newton ... Articles from Britannica Encyclopedias for elementary and high school students. Isaac Newton - Children's Encyclopedia (Ages 8-11) Isaac Newton - Student Encyclopedia (Ages 11 and up) Simplify &nbspThis&nbsp Article Ask the Chatbot a Question ... Richard S. Westfall · Professor of History of Science, Indiana University, Bloomington, 1963–89. Author of Never at Rest: A Biography of Isaac Newton and others. Richard S. Westfall ... Encyclopaedia Britannica's editors oversee subject areas in which they have extensive knowledge, whether from years of experience gained by working on that content or via study for an advanced degree. They write new content and verify and edit content received from contributors. ... Although Isaac Newton is well known for his discoveries in optics (white light composition) and mathematics (calculus), it is his formulation of the three laws of motion—the basic principles of modern\", 'timestamp': '1999-07-26T00:00:00', 'title': 'Isaac Newton | Biography, Facts, Discoveries, Laws, & Inventions | Britannica', 'url': 'https://www.britannica.com/biography/Isaac-Newton'}, {'snippet': 'Born in the hamlet of Woolsthorpe, Newton was the only son of a local yeoman, also Isaac Newton, who had died three months before, and of Hannah Ayscough. That same year, at Arcetri near Florence, Galileo Galilei had died; Newton would eventually pick up his idea of a mathematical science of motion and bring his work to full fruition. A tiny and weak baby, Newton was not expected to survive his first day of life, much less 84 years. Deprived of a father before birth, he soon lost his mother as well, for within two years she married a second time; her husband, the well-to-do minister Barnabas Smith, left young Isaac with his grandmother and moved to a neighbouring village to raise a son and two daughters. For nine years, until the death of Barnabas Smith in 1653, Isaac was effectively separated from his mother, and his pronounced psychotic tendencies have been ascribed to this traumatic event.', 'timestamp': '1999-07-26T00:00:00', 'title': 'Isaac Newton | Biography, Facts, Discoveries, Laws, & Inventions | Britannica', 'url': 'https://www.britannica.com/biography/Isaac-Newton'}, {'snippet': \"Isaac NewtonIsaac Newton, oil on canvas by Sir Godfrey Kneller, 1702; in the National Portrait Gallery, London. Newton's discoveries in physics and mathematics revolutionized science.(more) Isaac Newton (born December 25, 1642 [January 4, 1643, New Style], Woolsthorpe, Lincolnshire, England—died March 20 [March 31], 1727, London) was an English physicist and mathematician who was the culminating figure of the Scientific Revolution of the 17th century. In optics, his discovery of the composition of white light integrated the phenomena of colours into the science of light and laid the foundation for modern physical optics. In mechanics, his three laws of motion, the basic principles of modern physics, resulted in the formulation of the law of universal gravitation. In mathematics, he was the original discoverer of the infinitesimal calculus. Newton’s Philosophiae Naturalis Principia Mathematica (Mathematical Principles of Natural Philosophy, 1687) was one of the most important single wor\", 'timestamp': '1999-07-26T00:00:00', 'title': 'Isaac Newton | Biography, Facts, Discoveries, Laws, & Inventions | Britannica', 'url': 'https://www.britannica.com/biography/Isaac-Newton'}, {'snippet': 'When he examined the state of his soul in 1662 and compiled a catalog of sins in shorthand, he remembered “Threatning my father and mother Smith to burne them and the house over them.” The acute sense of insecurity that rendered him obsessively anxious when his work was published and irrationally violent when he defended it accompanied Newton throughout his life and can plausibly be traced to his early years. After his mother was widowed a second time, she determined that her first-born son should manage her now considerable property. It quickly became apparent, however, that this would be a disaster, both for the estate and for Newton. He could not bring himself to concentrate on rural affairs—set to watch the cattle, he would curl up under a tree with a book. Fortunately, the mistake was recognized, and Newton was sent back to the grammar school in Grantham, where he had already studied, to prepare for the university.', 'timestamp': '1999-07-26T00:00:00', 'title': 'Isaac Newton | Biography, Facts, Discoveries, Laws, & Inventions | Britannica', 'url': 'https://www.britannica.com/biography/Isaac-Newton'}, {'snippet': 'Sir Isaac Newton contributed significantly to the field of science over his lifetime. He invented · calculus and provided a clear understanding of optics. But his most significant work had to do with forces, and specifically with the development of a universal law of gravitation and his ... Isaac Newton was born on Christmas Day to a poor farming family in Woolsthorpe, England, in 1642. At the time of Newton\\'s birth England used the Julian calendar, however, when England adopted the Gregorian calendar in 1752, his birthday became 4th January 1643.  · Isaac Newton arrived in the world only a few months after his father, Isaac Newton Sr, had died. \"The boy expected to live managing the farm in the place of the father he had never known,\" wrote James Gleick in \"Isaac Newton\" (Vintage, 2004).  ... However, when it became clear a farming life was not for him, Newton attended Trinity College in Cambridge, England.', 'timestamp': '2023-06-06T15:20:35', 'title': 'Sir Isaac Newton biography — Inventions, laws and quotes | Space', 'url': 'https://www.space.com/15898-isaac-newton.html'}, {'snippet': 'A short history of Sir Isaac Newton, the mathematician and physicist that helped invent and explain some of the most fundamental laws of science.', 'timestamp': '2023-06-06T15:20:35', 'title': 'Sir Isaac Newton biography — Inventions, laws and quotes | Space', 'url': 'https://www.space.com/15898-isaac-newton.html'}, {'snippet': 'Sir Isaac Newton invented calculus and explained optics. His most significant work involved forces and the development of a universal law of gravity.', 'timestamp': '2023-06-06T15:20:35', 'title': 'Sir Isaac Newton biography — Inventions, laws and quotes | Space', 'url': 'https://www.space.com/15898-isaac-newton.html'}, {'snippet': \"What makes Newton's laws work? Here's the simple trick. The story of Isaac Newton and the apple tree may well be a self-created myth, but it might have helped his audience understand some of the concepts he was explaining. (Image credit: Photos.com via Getty Images) A popular myth tells of an apple falling from a tree in Newton's garden, which brought Newton to an understanding of forces, particularly gravity. Whether the incident actually happened is unknown, but historians doubt the event — if it occurred — was the driving force in Newton's thought process. The myth tells of Isaac Newton having returned to his family farm in Woolsthorpe, escaping Cambridge for a short time as it was dealing with a plague outbreak. As he sat in the farm's orchard, an apple fell from one of the trees (in some tellings it hit Newton on the head). Watching this happen, Newton began to consider the forces that meant the apple always fell directly towards the ground, beginning his examination of gravity.\", 'timestamp': '2023-06-06T15:20:35', 'title': 'Sir Isaac Newton biography — Inventions, laws and quotes | Space', 'url': 'https://www.space.com/15898-isaac-newton.html'}, {'snippet': 'Isaac Newton was born in 1642 in England. His father had died two months before his birth. When Isaac was three his mother remarried, and Isaac remained with his grandmother. He was not interested in the family farm, so he was sent to Cambridge University to study. Isaac was born just a short time after the death of Galileo, one of the greatest scientists of all time. Galileo had proved that the planets revolve around the sun, not the earth as people thought at the time. Isaac Newton was very interested in the discoveries of Galileo and others. Isaac thought the universe worked like a machine and that a few simple laws governed it. Like Galileo, he realized that mathematics was the way to explain and prove those laws. Isaac Newton was one of the world’s great scientists because he took his ideas, and the ideas of earlier scientists, and combined them into a unified picture of how the universe works.', 'timestamp': '', 'title': 'Newton (1642) - Energy Kids: U.S. Energy Information Administration (EIA)', 'url': 'https://www.eia.gov/kids/history-of-energy/famous-people/newton.php'}, {'snippet': 'Isaac Newton’s calculations changed the way people understood the universe. No one had been able to explain why the planets stayed in their orbits. What held them up? Less than 50 years before Isaac Newton was born it was thought that the planets were held in place by an invisible shield. Isaac proved that they were held in place by the sun’s gravity. He also showed that the force of gravity was affected by distance and by mass. He was not the first to understand that the orbit of a planet was not circular, but more elongated, like an oval.', 'timestamp': '', 'title': 'Newton (1642) - Energy Kids: U.S. Energy Information Administration (EIA)', 'url': 'https://www.eia.gov/kids/history-of-energy/famous-people/newton.php'}, {'snippet': 'Isaac Newton used three laws to explain the way objects move. They are often call Newton’s Laws. The First Law states that an object that is not being pushed or pulled by some force will stay still, or will keep moving in a straight line at a steady speed. It is easy to understand that a bike will not move unless something pushes or pulls it. It is harder to understand that an object will continue to move without help. Think of the bike again. If someone is riding a bike and jumps off before the bike is stopped what happens? The bike continues on until it falls over. The tendency of an object to remain still, or keep moving in a straight line at a steady speed is called inertia. The Second Law {force = mass x acceleration; f = ma} explains how a force acts on an object. An object accelerates in the direction the force is moving it. If someone gets on a bike and pushes the pedals forward the bike will begin to move.', 'timestamp': '', 'title': 'Newton (1642) - Energy Kids: U.S. Energy Information Administration (EIA)', 'url': 'https://www.eia.gov/kids/history-of-energy/famous-people/newton.php'}, {'snippet': 'When most people think of Isaac Newton, they think of him sitting under an apple tree observing an apple fall to the ground. When he saw the apple fall, Newton began to think about a specific kind of motion—gravity. Newton understood that gravity was the force of attraction between two objects. He also understood that an object with more matter –mass- exerted the greater force, or pulled smaller object toward it. That meant that the large mass of the earth pulled objects toward it. That is why the apple fell down instead of up, and why people don’t float in the air. Isaac thought about gravity and the apple. He thought that maybe gravity was not just limited to the earth and the objects on it. What if gravity extended to the moon and beyond? Isaac calculated the force needed to keep the moon moving around the earth. Then he compared it with the force the made the apple fall downward.', 'timestamp': '', 'title': 'Newton (1642) - Energy Kids: U.S. Energy Information Administration (EIA)', 'url': 'https://www.eia.gov/kids/history-of-energy/famous-people/newton.php'}, {'snippet': 'Isaac Newton was appointed Warden of the British Mint in 1695, and in 1703, he was elected president of the Royal Society and was re-elected every year until his death. In 1704, he published another influential work, the Opticks, which detailed the revolutionary experiments he had conducted on the properties of light and matter decades earlier. In 1705, Queen Anne conferred Knighthood on Isaac Newton. Ironically he was honored for his political work, not his mathematical or scientific accomplishments. Sir Isaac Newton died at Kensington, London on March 31, 1727, and was buried in Westminster Abbey, the first scientist to be accorded that honor. Asteroids 8000 Isaac Newton and 662 Newtonia are named in his honor, as is a crater on Mars. Asteroid 2653 Principia memorializes his monumental treatise, Philosophia Naturalis Principia Mathematica. One of the most eccentric geniuses in history, Sir Isaac Newton has been credited with contributing more to the development of modern science than', 'timestamp': '2023-03-08T23:35:45', 'title': 'Sir Isaac Newton - New Mexico Museum of Space History', 'url': 'https://nmspacemuseum.org/inductee/sir-isaac-newton/'}, {'snippet': 'British scientist and mathematician Sir Isaac Newton contributed substantially to the science of optics and the nature of light, formulated the laws of motion and gravitation, and independently developed a […]', 'timestamp': '2023-03-08T23:35:45', 'title': 'Sir Isaac Newton - New Mexico Museum of Space History', 'url': 'https://nmspacemuseum.org/inductee/sir-isaac-newton/'}, {'snippet': 'Newton returned to his family’s farm until the university reopened in late 1666 after the plague had passed. After receiving his Master’s degree in 1668 he remained at Cambridge until 1687, teaching and developing theories in mathematics and physics. During this time he invented the branch of mathematics called infinitesimal calculus and identified many of the properties of light and optics. In 1668, Isaac Newton also built the first reflecting telescope, a major improvement on earlier designs. Beset by emotional problems his entire life, and apparently afflicted by mercury poisoning due to his experiments in alchemy (the discredited pseudo-science centered on turning lead into gold), he suffered an emotional breakdown in 1678. By the next year, he had recovered enough to return to his research in celestial mechanics, as well as his less scientific pursuit of alchemy.', 'timestamp': '2023-03-08T23:35:45', 'title': 'Sir Isaac Newton - New Mexico Museum of Space History', 'url': 'https://nmspacemuseum.org/inductee/sir-isaac-newton/'}])]), ChatbotMessage(role='CHATBOT', message=\"Sir Isaac Newton was born on December 25, 1642. At the time of Newton's birth, England used the Julian calendar. However, when England adopted the Gregorian calendar in 1752, his birthday became January 4, 1643.\", tool_calls=None)], meta=ApiMeta(api_version=ApiMetaApiVersion(version='1', is_deprecated=None, is_experimental=None), billed_units=ApiMetaBilledUnits(images=None, input_tokens=946.0, output_tokens=74.0, search_units=None, classifications=None), tokens=ApiMetaTokens(input_tokens=2539.0, output_tokens=124.0), warnings=None))"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response= co.chat(\n",
    "    chat_history= chat_history , \n",
    "    message= message ,\n",
    "    ## to perform web search before answering we can use our own connectors \n",
    "    connectors= [{\"id\": \"web-search\"}]\n",
    "    \n",
    ")\n",
    "\n",
    "response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "09c3718f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Sir Isaac Newton was born on December 25, 1642. At the time of Newton's birth, England used the Julian calendar. However, when England adopted the Gregorian calendar in 1752, his birthday became January 4, 1643.\""
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6517e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM , AutoTokenizer \n",
    "import torch \n",
    "\n",
    "#downloade model \n",
    "model_id= \"meta-llama/Llama-2-7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "6a49322f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4574033207346f7a75e8696b13470ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.62k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b97b32d5dcd4376a67523c251cf84f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d50d081a892048499032122ef1ffd4ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f6de8a3b2df47f19609aa06d593793f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75a1633b219e4106b226ca94d9a05ff2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/614 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f46f117aa3741b68a1a0abca8fc3a68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "820254b57b874adebde8ecdd1a0d0c6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3186e61683a746f3a4053ae2c3fae4ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c90ace6212064c298b5599985473c02a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a99e1dd68c9e4c9292d0acb04eb59a5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OSError",
     "evalue": "The paging file is too small for this operation to complete. (os error 1455)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[125]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m cache_dir = \u001b[33m\"\u001b[39m\u001b[33mD:/transformers_cache\u001b[39m\u001b[33m\"\u001b[39m  \u001b[38;5;66;03m# Set this to any folder on your D drive\u001b[39;00m\n\u001b[32m      7\u001b[39m tokenizer = AutoTokenizer.from_pretrained(model_id, cache_dir=cache_dir)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m model = \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbfloat16\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\ujjwal\\Building-LLMs-for-Production\\.venv\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:600\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    598\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m model_class.config_class == config.sub_configs.get(\u001b[33m\"\u001b[39m\u001b[33mtext_config\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    599\u001b[39m         config = config.get_text_config()\n\u001b[32m--> \u001b[39m\u001b[32m600\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    601\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    602\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    603\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    604\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig.\u001b[34m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    605\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(c.\u001b[34m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m._model_mapping.keys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    606\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\ujjwal\\Building-LLMs-for-Production\\.venv\\Lib\\site-packages\\transformers\\modeling_utils.py:311\u001b[39m, in \u001b[36mrestore_default_torch_dtype.<locals>._wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    309\u001b[39m old_dtype = torch.get_default_dtype()\n\u001b[32m    310\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m311\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    312\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    313\u001b[39m     torch.set_default_dtype(old_dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\ujjwal\\Building-LLMs-for-Production\\.venv\\Lib\\site-packages\\transformers\\modeling_utils.py:4839\u001b[39m, in \u001b[36mPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[39m\n\u001b[32m   4829\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m dtype_orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   4830\u001b[39m         torch.set_default_dtype(dtype_orig)\n\u001b[32m   4832\u001b[39m     (\n\u001b[32m   4833\u001b[39m         model,\n\u001b[32m   4834\u001b[39m         missing_keys,\n\u001b[32m   4835\u001b[39m         unexpected_keys,\n\u001b[32m   4836\u001b[39m         mismatched_keys,\n\u001b[32m   4837\u001b[39m         offload_index,\n\u001b[32m   4838\u001b[39m         error_msgs,\n\u001b[32m-> \u001b[39m\u001b[32m4839\u001b[39m     ) = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_load_pretrained_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4840\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4841\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4842\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcheckpoint_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4843\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4844\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4845\u001b[39m \u001b[43m        \u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4846\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4847\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdisk_offload_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4848\u001b[39m \u001b[43m        \u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4849\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4850\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4851\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4852\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4853\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkey_mapping\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkey_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4854\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4855\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4857\u001b[39m \u001b[38;5;66;03m# record tp degree the model sharded to\u001b[39;00m\n\u001b[32m   4858\u001b[39m model._tp_size = tp_size\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\ujjwal\\Building-LLMs-for-Production\\.venv\\Lib\\site-packages\\transformers\\modeling_utils.py:5302\u001b[39m, in \u001b[36mPreTrainedModel._load_pretrained_model\u001b[39m\u001b[34m(cls, model, state_dict, checkpoint_files, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, device_map, disk_offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_regex, device_mesh, key_mapping, weights_only)\u001b[39m\n\u001b[32m   5299\u001b[39m         args_list = logging.tqdm(args_list, desc=\u001b[33m\"\u001b[39m\u001b[33mLoading checkpoint shards\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   5301\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m args \u001b[38;5;129;01min\u001b[39;00m args_list:\n\u001b[32m-> \u001b[39m\u001b[32m5302\u001b[39m         _error_msgs, disk_offload_index, cpu_offload_index = \u001b[43mload_shard_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5303\u001b[39m         error_msgs += _error_msgs\n\u001b[32m   5305\u001b[39m \u001b[38;5;66;03m# Adjust offloaded weights name and save if needed\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\ujjwal\\Building-LLMs-for-Production\\.venv\\Lib\\site-packages\\transformers\\modeling_utils.py:920\u001b[39m, in \u001b[36mload_shard_file\u001b[39m\u001b[34m(args)\u001b[39m\n\u001b[32m    918\u001b[39m \u001b[38;5;66;03m# If shard_file is \"\", we use the existing state_dict instead of loading it\u001b[39;00m\n\u001b[32m    919\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m shard_file != \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m920\u001b[39m     state_dict = \u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    921\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshard_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_quantized\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_quantized\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweights_only\u001b[49m\n\u001b[32m    922\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    924\u001b[39m \u001b[38;5;66;03m# Fix the key names\u001b[39;00m\n\u001b[32m    925\u001b[39m state_dict = {key_renaming_mapping[k]: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m state_dict.items() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m key_renaming_mapping}\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\ujjwal\\Building-LLMs-for-Production\\.venv\\Lib\\site-packages\\transformers\\modeling_utils.py:532\u001b[39m, in \u001b[36mload_state_dict\u001b[39m\u001b[34m(checkpoint_file, is_quantized, map_location, weights_only)\u001b[39m\n\u001b[32m    530\u001b[39m \u001b[38;5;66;03m# Use safetensors if possible\u001b[39;00m\n\u001b[32m    531\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m checkpoint_file.endswith(\u001b[33m\"\u001b[39m\u001b[33m.safetensors\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m is_safetensors_available():\n\u001b[32m--> \u001b[39m\u001b[32m532\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43msafe_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframework\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m    533\u001b[39m         metadata = f.metadata()\n\u001b[32m    535\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m metadata \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m metadata.get(\u001b[33m\"\u001b[39m\u001b[33mformat\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mtf\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mflax\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmlx\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\u001b[31mOSError\u001b[39m: The paging file is too small for this operation to complete. (os error 1455)"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "model_id = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "cache_dir = \"D:/transformers_cache\"  # Set this to any folder on your D drive\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, cache_dir=cache_dir)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    cache_dir=cache_dir\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdac439b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f212a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2e067b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2328000c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Building-LLMs-for-Production",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
